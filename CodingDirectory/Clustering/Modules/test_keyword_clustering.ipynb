{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eea13db-7037-4773-9e9e-92f3ab137815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from itertools import chain, groupby\n",
    "from operator import itemgetter\n",
    "from sklearn.cluster import DBSCAN\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "from solrhandler import SolrHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6f1f0-00f5-47ac-ae04-2b8beee958a1",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa32b25a-9f78-4178-a67d-5b7b0a0f7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, nlp_model):\n",
    "    doc = nlp_model(text)\n",
    "    text = \" \".join([tok.lemma_ for tok in doc if not tok.is_stop and tok.is_alpha])\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddb7f5e-1e0b-4250-8b43-4f7b44cc85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tokens(tokens):\n",
    "    tokens_unique = []\n",
    "    token_texts_unique= []\n",
    "    for tok in tokens:\n",
    "        if tok.text not in token_texts_unique:\n",
    "            tokens_unique.append(tok)\n",
    "            token_texts_unique.append(tok.text)\n",
    "    return tokens_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8678eb9-6582-46fd-9c48-cd88f838210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_lst(lst):\n",
    "    return list(chain(*lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b696494-e9ca-472a-b1af-76c2b57725c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2cluster_topic(clusters):\n",
    "    word2cluster_topic = {}\n",
    "    for cluster in clusters:\n",
    "        for elem in cluster:\n",
    "            word2cluster_topic.update({elem: cluster[0]})\n",
    "    return word2cluster_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7aab5f1a-511f-4833-b2bc-d121fa925cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(lst):\n",
    "    return list(set(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cb721b5-8158-40ed-a484-96a4d47d0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(tokens, clustering):\n",
    "    vecs = [tok.vector for tok in tokens]\n",
    "    labels = clustering.fit_predict(vecs)\n",
    "    elems = [(tok.text, label) for tok, label in zip(tokens, labels)]\n",
    "    clusters = [[el[0] for el in elems if el[1] == i] for i in range(len(set(labels)))]\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4affd995-7b6b-493c-93ab-7edbb6be225b",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af5e1541-b0c4-4947-9734-c03d9319a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72131ceb-d5a1-47df-87f7-5617559fb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = SolrHandler(max_elems=20000)\n",
    "df = handler.get_df_from_query(\"*%3A*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d53fb388-c24a-492b-81b8-68d3451b0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_lst = [\" \".join(lst) for lst in df.ssdsLemma.tolist()]\n",
    "lemma_lst = [preprocess_text(lemma_str, nlp) for lemma_str in lemma_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd80f7fa-b7e1-4d86-9db3-2b9fca5b50b5",
   "metadata": {},
   "source": [
    "## Keyword Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbac4442-0c89-40da-91f2-11f1db2ca025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_clustered(lemma_lst, nlp_model, clustering=DBSCAN(eps=.15, min_samples=1, metric=\"cosine\")):\n",
    "    lemmas_tokens = [[tok for tok in doc] for doc in nlp_model.pipe(lemma_lst)]\n",
    "    lemma_tokens_lst = get_unique_tokens(flatten_lst(lemmas_tokens))\n",
    "    tokens_vectorizable = [tok for tok in lemma_tokens_lst if tok.has_vector]\n",
    "    tokens_not_vectorizable = [tok for tok in lemma_tokens_lst if tok not in tokens_vectorizable]\n",
    "    clusters = get_clusters(tokens_vectorizable, clustering)\n",
    "    word2cluster_topic = get_word2cluster_topic(clusters)\n",
    "    word2cluster_topic.update({tok.text: tok.text for tok in tokens_not_vectorizable})\n",
    "    return [drop_duplicates([word2cluster_topic[tok.text] for tok in res]) for res in lemmas_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63634425-affb-4747-b4b7-c0c4e400c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=.3, min_samples=1, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b67b955f-ff60-4dbf-bb9b-5027f556eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_clustered = get_keywords_clustered(lemma_lst, nlp, clustering)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
